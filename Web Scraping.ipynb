{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d9f16f",
   "metadata": {},
   "source": [
    "## Scraping Top Repositories for Topics on GitHub\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2fb833",
   "metadata": {},
   "source": [
    "### What is Web Scraping?\n",
    "\n",
    "Web scraping is a method used to automatically extract data from websites. It involves retrieving HTML content from web pages and then parsing and extracting the desired information. This technique is valuable for various purposes, including data collection, content aggregation, monitoring, automation, and building APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0752cc",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "GitHub is a widely used platform for hosting and collaborating on software projects using version control. However, navigating and extracting specific information from GitHub's vast repository of projects can be time-consuming. In this project, we aim to address this issue by utilizing web scraping techniques to extract data from GitHub's topics page. Our goal is to automatically gather information about topics and their top repositories, including repository name, username, stars, and repository URL. By automating this process, we can streamline data collection and make it more efficient for various purposes, such as research, analysis, and monitoring trends in software development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b9bbf",
   "metadata": {},
   "source": [
    "### Tools used: Python, requests, Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ebea4",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "- Scrape the GitHub topics page to retrieve a list of topics.\n",
    "- For each topic, extract the topic title, topic page URL, and topic description.\n",
    "- For each topic, navigate to its page and scrape the top 25 repositories within that topic.\n",
    "- For each repository, extract the repository name, username, number of stars, and repository URL.\n",
    "- Compile this information into a CSV file with the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6bbc4",
   "metadata": {},
   "source": [
    "### Scrape the list of topics from Github\n",
    "\n",
    "- use requests to downlaod the page\n",
    "- user BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe\n",
    "\n",
    "Let's write a function to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5b59e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topic_page():\n",
    "    \n",
    "    topic_url = 'https://github.com/topics'\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page {topic_page_url}\")\n",
    "    \n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')  \n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2be21253",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc = get_topic_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261dccc",
   "metadata": {},
   "source": [
    "Let's create some helper functions to parse information from the page.\n",
    "\n",
    "To get topic titles, we can pick p tags with the class ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0966092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    topic_title = doc.find_all('p', class_='f3 lh-condensed mb-0 mt-1 Link--primary')\n",
    "    topic_titles = []\n",
    "    \n",
    "    for tag in topic_title:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b728f5",
   "metadata": {},
   "source": [
    "`get_topic_titles` can be used to get the list of titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2d622",
   "metadata": {},
   "source": [
    "Similarly we have defined functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "79dd52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    topic_description = doc.find_all('p', class_='f5 color-fg-muted mb-0 mt-1')\n",
    "    topic_desc = []\n",
    "    \n",
    "    for tag in topic_description:\n",
    "        topic_desc.append(tag.text.strip())\n",
    "    return topic_desc\n",
    "\n",
    "def get_topic_urls(doc):\n",
    "    topic_link = doc.find_all('a', class_='no-underline flex-1 d-flex flex-column')\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    \n",
    "    for url in topic_title:\n",
    "        topic_urls.append(base_url + url.parent['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd765b69",
   "metadata": {},
   "source": [
    "Let's put this all together into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6064cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page {topics_url}\")\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658d824",
   "metadata": {},
   "source": [
    "### Get the top 20 repositories from a topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8aab593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page {topic_page_url}\")\n",
    "    \n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')  \n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "23ed7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f9d92a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ratings_count(ratings):\n",
    "    ratings = ratings.text\n",
    "    if ratings[-1] == 'k':\n",
    "        return int(float(ratings[:-1]) * 1000)\n",
    "    return int(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d1a37610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(repo, ratings):\n",
    "    a_tags = repo.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    url = base_url + a_tags[1]['href']\n",
    "    rating = parse_ratings_count(ratings)\n",
    "    return username, repo_name, rating, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6e939dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \n",
    "    # Get the h3 tags containing repo title, repo URL and username\n",
    "    repo = topic_doc.find_all('h3',class_='f3 color-fg-muted text-normal lh-condensed')\n",
    "    \n",
    "    # Get ratings tags\n",
    "    ratings = topic_doc.find_all('span',class_='Counter js-social-count')\n",
    "    \n",
    "\n",
    "    repos_dict = {\n",
    "        'username': [],\n",
    "        'repo_name': [],\n",
    "        'stars': [],\n",
    "        'repo_url': []\n",
    "    }\n",
    "    \n",
    "    # Get repo info\n",
    "    for i in range(len(repo)):\n",
    "        repo_info = get_repo_info(repo[i], ratings[i])\n",
    "        repos_dict['username'].append(repo_info[0])\n",
    "        repos_dict['repo_name'].append(repo_info[1])\n",
    "        repos_dict['stars'].append(repo_info[2])\n",
    "        repos_dict['repo_url'].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d43862fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scrape_topic(url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The file {path} already exists. Skipping...\")\n",
    "    topic_df = get_topic_repos(get_topic_page(url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e41a45",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "- We have a funciton to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Let's create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f8222037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print(f\"Scraping top repositories for \"{row['title']}\"\")\n",
    "        scrape_topic(row['url'], 'data/{}'.format(row['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13917658",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7613c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for 3D\n",
      "The file data/3D already exists. Skipping...\n",
      "Scraping top repositories for Ajax\n",
      "The file data/Ajax already exists. Skipping...\n",
      "Scraping top repositories for Algorithm\n",
      "The file data/Algorithm already exists. Skipping...\n",
      "Scraping top repositories for Amp\n",
      "The file data/Amp already exists. Skipping...\n",
      "Scraping top repositories for Android\n",
      "The file data/Android already exists. Skipping...\n",
      "Scraping top repositories for Angular\n",
      "The file data/Angular already exists. Skipping...\n",
      "Scraping top repositories for Ansible\n",
      "The file data/Ansible already exists. Skipping...\n",
      "Scraping top repositories for API\n",
      "The file data/API already exists. Skipping...\n",
      "Scraping top repositories for Arduino\n",
      "The file data/Arduino already exists. Skipping...\n",
      "Scraping top repositories for ASP.NET\n",
      "The file data/ASP.NET already exists. Skipping...\n",
      "Scraping top repositories for Atom\n",
      "The file data/Atom already exists. Skipping...\n",
      "Scraping top repositories for Awesome Lists\n",
      "The file data/Awesome Lists already exists. Skipping...\n",
      "Scraping top repositories for Amazon Web Services\n",
      "The file data/Amazon Web Services already exists. Skipping...\n",
      "Scraping top repositories for Azure\n",
      "The file data/Azure already exists. Skipping...\n",
      "Scraping top repositories for Babel\n",
      "The file data/Babel already exists. Skipping...\n",
      "Scraping top repositories for Bash\n",
      "The file data/Bash already exists. Skipping...\n",
      "Scraping top repositories for Bitcoin\n",
      "The file data/Bitcoin already exists. Skipping...\n",
      "Scraping top repositories for Bootstrap\n",
      "The file data/Bootstrap already exists. Skipping...\n",
      "Scraping top repositories for Bot\n",
      "The file data/Bot already exists. Skipping...\n",
      "Scraping top repositories for C\n",
      "The file data/C already exists. Skipping...\n",
      "Scraping top repositories for Chrome\n",
      "The file data/Chrome already exists. Skipping...\n",
      "Scraping top repositories for Chrome extension\n",
      "The file data/Chrome extension already exists. Skipping...\n",
      "Scraping top repositories for Command line interface\n",
      "The file data/Command line interface already exists. Skipping...\n",
      "Scraping top repositories for Clojure\n",
      "The file data/Clojure already exists. Skipping...\n",
      "Scraping top repositories for Code quality\n",
      "The file data/Code quality already exists. Skipping...\n",
      "Scraping top repositories for Code review\n",
      "The file data/Code review already exists. Skipping...\n",
      "Scraping top repositories for Compiler\n",
      "The file data/Compiler already exists. Skipping...\n",
      "Scraping top repositories for Continuous integration\n",
      "The file data/Continuous integration already exists. Skipping...\n",
      "Scraping top repositories for COVID-19\n",
      "The file data/COVID-19 already exists. Skipping...\n",
      "Scraping top repositories for C++\n",
      "The file data/C++ already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "scrape_topic_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3270a",
   "metadata": {},
   "source": [
    "We can check that the CSVs were created properly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
